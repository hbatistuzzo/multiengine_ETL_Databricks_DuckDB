# Multiengine ETL with Databricks and DuckDB

![GitHub top language](https://img.shields.io/github/languages/top/hbatistuzzo/multiengine_ETL_Databricks_DuckDB)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/hbatistuzzo/multiengine_ETL_Databricks_DuckDB)
![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/hbatistuzzo/multiengine_ETL_Databricks_DuckDB)
![GitHub last commit](https://img.shields.io/github/last-commit/hbatistuzzo/multiengine_ETL_Databricks_DuckDB)


The goal of this project is to practice the use of Databricks and DuckDB in constructing ETL pipelines. It is based on [lvgalvao`s](https://github.com/lvgalvao) 5-day bootcamp on the subject that happened from 2025/01/28 to 2025/01/31.

We`re working with 3 "data ingestion models": one daily, one hourly and one real-time (every 15 seconds), either through API's or RMDB's.

The original bootcamp uses Databricks' enterprise verion to test some more advanced features. In this project, we'll go with the community version, lest AWS syphons my entire back account with hidden background-running services, taxes, and whatnot.




<br>

<table align="center" style="border-collapse: collapse;">
  <tr>
    <td align="center" style="border: none;">
      <img alt="Databricks" height="100" src="images/bricks.png"/>
      <br> Databricks
    </td>
    <td align="center" style="border: none;">
      <img src="images/DuckDB.png" alt="duckdb" height="100"/>
      <br>DuckDB
    </td>
  </tr>
</table>





<br>

<div align="center">

# $\color{goldenrod}{\textrm{Day 1 - Intro}}$

</div>

<div align="center">

## $\color{goldenrod}{\textrm{1.1 - Intro}}$

</div>

Databricks offers a number of advantages when dealing with large volumes of data: not only does it leverages functionalities from Spark - by enabling distributed processing - but it also offers in-memory processing and synergizes well with any storaging solution.

Spark itself has evolved its ecossystem: SparkSQL for querying, SparkMLlib for machine learning, SparkStreaming for low latency. Spark Dataframes interface with a number of languages nowadays: SQL, R, Scala, Java, Python, etc; the more specialized _Datasets_ only work with Scala, but that's for very specific cases.

Spark itself appeared as an open-sourcer processing solution to gain space in the Data Engineering market. Databricks, on the other had, aims to gain the market by selling itself directly to the executive team. It's a ready-to-use toolbox, and it adds new features constantly.
<br>

<div align="center">

## $\color{goldenrod}{\textrm{1.2 - How does it work?}}$

</div>




<br>
<br>

<div align="center">

# $\color{goldenrod}{\textrm{Day 2 - }}$

</div>

<div align="center">

## $\color{goldenrod}{\textrm{2.1 - Intro}}$

</div>



<br>
<br>
<br>

<div align="center">

# $\color{goldenrod}{\textrm{Day 3 - }}$

</div>

<div align="center">

## $\color{goldenrod}{\textrm{3.1 - Intro}}$

</div>






<br>
<br>
<br>

<div align="center">

# $\color{goldenrod}{\textrm{Day 4 - }}$

</div>

<div align="center">

## $\color{goldenrod}{\textrm{4.1 - Intro}}$

</div>






<br>
<br>
<br>

<div align="center">

# $\color{goldenrod}{\textrm{Day 5 - }}$

</div>

<div align="center">

## $\color{goldenrod}{\textrm{5.1 - Intro}}$

</div>
